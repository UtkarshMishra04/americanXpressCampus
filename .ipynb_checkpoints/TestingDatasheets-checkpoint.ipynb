{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading complete\n",
      "data application_key dropped\n",
      "data mvar47 dropped\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#Reading data\n",
    "\n",
    "df_training = pd.read_csv(\"Training_dataset_Original.csv\")\n",
    "df_eval = pd.read_csv(\"Evaluation_dataset.csv\")\n",
    "df_leader = pd.read_csv(\"Leaderboard_dataset.csv\")\n",
    "df_datadict = pd.read_csv(\"Data_Dictionary.csv\")\n",
    "\n",
    "print(\"data reading complete\")\n",
    "\n",
    "df_training = df_training.drop(['application_key'],axis=1)\n",
    "\n",
    "print(\"data application_key dropped\")\n",
    "\n",
    "mvar47 = df_training.loc[:, 'mvar47']\n",
    "\n",
    "df_training = df_training.drop(['mvar47'],axis=1)\n",
    "\n",
    "print(\"data mvar47 dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processed\n",
      "      mvar1   mvar2   mvar3  mvar4  mvar5  mvar6   mvar7   mvar8   mvar9  \\\n",
      "0      1696  1.6541   0.000  0.000  0.000      0    6015     322   40369   \n",
      "1      1846  0.8095   0.000  0.000  0.000    102    7532    3171   18234   \n",
      "2      1745  0.4001   0.000  0.000  0.000      0    2536       0       0   \n",
      "3      1739  0.2193   0.000  0.000  0.000   1982   26440    4955   20316   \n",
      "4      1787  0.0118   0.225  0.000  0.000   5451    5494    5494    7987   \n",
      "5      1579  0.0000   3.502  0.000  0.000      0       0       0       0   \n",
      "6      1818  0.4001   0.000  0.000  0.000      0    1088       0    1536   \n",
      "7         0  0.0000   0.000  0.000  0.000      0       0       0       0   \n",
      "8      1836  0.1358   0.000  0.000  0.000    347   38964   17828   70729   \n",
      "9      1839  0.1981   0.000  0.000  0.000    793    6131    6045   48959   \n",
      "10     1903  0.0000   0.000  0.000  0.000     22   19518    9910   90618   \n",
      "11     1681  1.5888   5.685  3.566  0.000     93    3554     439    2527   \n",
      "12     1841  0.5388   0.000  0.000  0.000   1487   19449   19449    6739   \n",
      "13     1794  1.9684   0.000  0.000  0.000    856    8609    8609   60545   \n",
      "14     1669  3.8558  18.465  6.292  1.200      0     142       0      38   \n",
      "15     1583  3.0284  16.325  0.000  0.000      0       0       0   51450   \n",
      "16     1899  0.0970   0.000  0.000  0.000   1015   13379   13379  246906   \n",
      "17     1690  8.7747  10.437  0.000  0.000    122     623     299    1140   \n",
      "18     1599  0.8601   1.525  1.525  0.000      0     878       0       0   \n",
      "19     1859  0.0155   0.368  0.000  0.000    424   86509   12451   86635   \n",
      "20     1880  2.6101   0.000  0.000  0.000      0   19252    6342   59601   \n",
      "21     1852  3.6201   0.000  0.000  0.000    799   16515   16515  110651   \n",
      "22     1655  3.0268  14.748  0.000  0.000     48      48      48     198   \n",
      "23     1863  0.0401   0.000  0.000  0.000   1487    4955    4955   22902   \n",
      "24     1753  0.2251   0.000  0.000  0.000   1124    1124    1124       0   \n",
      "25     1894  0.0318   0.000  0.000  0.000   4655    7433    7433   25667   \n",
      "26     1803  0.3993   0.000  0.000  0.000    496   10505     496     991   \n",
      "27     1932  0.3276   0.000  0.000  0.000   6982   21569   18829  187209   \n",
      "28     1714  5.1904   0.770  0.000  0.000      0    1889     595   12574   \n",
      "29        0  2.0457   4.514  0.000  0.000      0       0       0       0   \n",
      "...     ...     ...     ...    ...    ...    ...     ...     ...     ...   \n",
      "79970  1834  1.2164   0.000  0.000  0.000   2212    5244    4203   16743   \n",
      "79971  1729  1.3696   0.000  0.000  0.000    356    4218    1982   18657   \n",
      "79972  1753  5.8283   0.000  0.000  0.000    180    4014     617    3667   \n",
      "79973  1748  1.9635   0.642  0.000  0.000    291    3469    3469   13131   \n",
      "79974  1771  1.7588   0.000  0.000  0.000     66   12626    6433   27946   \n",
      "79975  1766  0.9540   0.000  0.000  0.000   1140   10207   10207   55001   \n",
      "79976  1822  0.0000   1.853  1.853  0.000   2581   13361    5946   31018   \n",
      "79977  1824  0.5634   0.000  0.000  0.000   3964    4683    4683       0   \n",
      "79978  1914  0.0544   0.000  0.000  0.000   3469   20424   20424   78774   \n",
      "79979  1600  2.6649  18.282  1.078  2.678      0    7205       0     956   \n",
      "79980  1822  0.5851   0.000  0.000  0.000      0   23685   23685  208174   \n",
      "79981  1693  0.1828  17.767  2.024  0.000    114   27514     384    2329   \n",
      "79982  1735  3.1701   0.260  0.000  0.000    117     892     117    3766   \n",
      "79983  1671  0.1380   0.000  0.000  0.000      0     491       0       0   \n",
      "79984  1924  0.1221   0.000  0.000  0.000   2478  266683  266683  405300   \n",
      "79985  1670  0.0347   9.836  0.000  0.000    212    7848     491   29785   \n",
      "79986  1901  0.1782   0.000  0.000  0.000   4955   21753   12673   70293   \n",
      "79987  1835  0.5372   0.000  0.000  0.000      0   29723    3964   66770   \n",
      "79988  1927  0.0410   0.000  0.000  0.000  22000   30046   24165   75019   \n",
      "79989     0  0.1451   0.000  0.000  0.000      0       0       0       0   \n",
      "79990  1941  0.4834   0.000  0.000  0.000  10846   52806   35478  136461   \n",
      "79991  1899  0.0000   0.000  0.000  0.000   7310   14519   14519    5252   \n",
      "79992  1761  2.0999   0.000  0.000  0.000     47   13477    1814    9320   \n",
      "79993  1700  0.3831   0.000  0.000  0.000    344     344     344     991   \n",
      "79994  1910  0.0000   0.000  0.000  0.000   4955   47306   47306   88124   \n",
      "79995  1736  2.1740   0.000  0.000  0.000     11    4248    1577   13379   \n",
      "79996  1724  0.0000   1.108  0.768  0.000      0   64041       0   10926   \n",
      "79997  1605  0.2901  11.561  0.937  2.976      0    2277       0    3964   \n",
      "79998  1780  1.1874   0.000  0.000  0.000      0    6356    4802    3206   \n",
      "79999  1727  1.9288   1.441  0.000  0.000      0   25773    2869  132985   \n",
      "\n",
      "       mvar10     ...     mvar38 mvar39   mvar40  mvar41   mvar42 mvar43  \\\n",
      "0       18414     ...          4      1    73.78  82.547  0.08696     10   \n",
      "1       13664     ...          2      0   99.129       0        0     13   \n",
      "2        2536     ...          1      0        0   29.29        0      1   \n",
      "3       37013     ...          2      0   96.272       0  0.15385      3   \n",
      "4        4696     ...          2      0  115.019       0        0      1   \n",
      "5           0     ...          2      0        0       0      1.5      0   \n",
      "6        1498     ...          0      0   88.171       0        0      2   \n",
      "7           0     ...          0      0        0       0        0      0   \n",
      "8       65843     ...          2      0        0       0        0     10   \n",
      "9       31640     ...          0      0        0   45.59  0.08824     14   \n",
      "10     110271     ...          2      0        0       0        0     17   \n",
      "11       7086     ...          5      1        0       0  0.61111      7   \n",
      "12       6612     ...          6      0        0       0        0      5   \n",
      "13      24893     ...          1      0        0       0        0     13   \n",
      "14        142     ...         15      0  107.825  98.664     0.56     11   \n",
      "15          0     ...          4      0        0       0  0.32353      2   \n",
      "16      56533     ...          2      0   10.384       0        0     19   \n",
      "17       2043     ...         12      0        0       0  0.63636      5   \n",
      "18        878     ...          1      1        0  91.175      0.8      0   \n",
      "19     142081     ...          2      0        0       0        0     12   \n",
      "20      86906     ...         14      0        0  72.497        0     14   \n",
      "21      41311     ...          2      0        0       0        0     13   \n",
      "22         48     ...          4      0        0       0        1      1   \n",
      "23      21772     ...          4      0   96.895       0        0     12   \n",
      "24       1124     ...          1      0        0       0        0      0   \n",
      "25      12345     ...          1      0        0       0        0      3   \n",
      "26      18163     ...          1      0   91.752  87.306      0.2      5   \n",
      "27     113813     ...          2      0        0       0        0     11   \n",
      "28       4825     ...          8      0        0       0  0.35714      6   \n",
      "29          0     ...          4      0        0       0  1.16667      0   \n",
      "...       ...     ...        ...    ...      ...     ...      ...    ...   \n",
      "79970   13831     ...          8      0        0  97.398        0      8   \n",
      "79971    8921     ...          7      0        0  83.663  0.09091      6   \n",
      "79972    9786     ...         10      0        0  87.094  0.11111      9   \n",
      "79973   14393     ...         12      0        0  92.244     0.44     14   \n",
      "79974   24401     ...          4      0        0  40.477        0     10   \n",
      "79975   20636     ...         13      0        0       0     0.05     14   \n",
      "79976   76774     ...          4      0   73.611  44.656  0.03448     20   \n",
      "79977    8647     ...          2      0        0       0        0      0   \n",
      "79978   27856     ...          1      0        0       0        0      8   \n",
      "79979    7360     ...          9      1  132.335  56.867  0.55263     14   \n",
      "79980   55719     ...          3      0        0       0        0     17   \n",
      "79981   48080     ...          4      0   74.814       0  0.17857      5   \n",
      "79982    1265     ...          3      0        0  99.026        0      2   \n",
      "79983     491     ...          1      0        0       0      0.8      2   \n",
      "79984  390558     ...          4      0        0  70.799        0     13   \n",
      "79985   16226     ...          5      0        0    78.7  0.13636     10   \n",
      "79986   62980     ...          5      0        0  61.301        0     10   \n",
      "79987   85900     ...          3      0        0       0        0      9   \n",
      "79988   88343     ...          3      0        0  19.444        0      7   \n",
      "79989       0     ...          1      0        0       0      1.5      0   \n",
      "79990  176240     ...          1      0        0       0  0.05882      8   \n",
      "79991    4949     ...          1      0        0       0        0      3   \n",
      "79992    2500     ...          5      0        0       0    0.125      5   \n",
      "79993     344     ...          3      0        0       0  0.66667      2   \n",
      "79994  107148     ...          2      0        0       0        0      4   \n",
      "79995    6671     ...          4      0        0  78.378        0      4   \n",
      "79996   84839     ...          5      0        0  38.325  0.16667     14   \n",
      "79997    5709     ...          5      2   101.85  93.142      0.5      4   \n",
      "79998   18180     ...          8      0        0  77.022  0.06061      9   \n",
      "79999   71788     ...          4      0     32.4       0  0.07143     12   \n",
      "\n",
      "        mvar44 mvar45 mvar46 default_ind  \n",
      "0      0.63899      0      0           0  \n",
      "1      0.63836      0      0           1  \n",
      "2      1.00000      0      0           1  \n",
      "3      0.53241      0      0           0  \n",
      "4      0.92665      0      0           0  \n",
      "5      0.00000      0      0           1  \n",
      "6      0.87224      0      0           1  \n",
      "7      0.00000      0      0           0  \n",
      "8      0.89868      0      0           0  \n",
      "9      0.33834      0      0           0  \n",
      "10     0.80620      0      0           0  \n",
      "11     0.81172      0      0           0  \n",
      "12     0.98031      0      0           0  \n",
      "13     0.35724      0      0           0  \n",
      "14     0.29169      0      1           0  \n",
      "15     0.00000      0      0           0  \n",
      "16     0.47480      0      0           0  \n",
      "17     0.94951      0      0           0  \n",
      "18     1.00000      0      0           1  \n",
      "19     0.34730      0      0           0  \n",
      "20     0.28708      0      0           0  \n",
      "21     0.75349      0      0           0  \n",
      "22     1.00000      0      0           1  \n",
      "23     0.40853      0      0           0  \n",
      "24     1.00000      0      0           1  \n",
      "25     0.99419      0      0           0  \n",
      "26     0.75289      0      0           0  \n",
      "27     0.99710      0      0           0  \n",
      "28     0.85082      0      0           0  \n",
      "29     0.00000      0      0           0  \n",
      "...        ...    ...    ...         ...  \n",
      "79970  0.58040      0      0           0  \n",
      "79971  0.62632      0      0           0  \n",
      "79972  0.59364      0      0           0  \n",
      "79973  0.83809      0      0           0  \n",
      "79974  0.46863      0      0           0  \n",
      "79975  0.54754      0      0           0  \n",
      "79976  0.23177      0      0           0  \n",
      "79977  0.67222      0      0           0  \n",
      "79978  1.00000      0      0           0  \n",
      "79979  0.11763      0      0           1  \n",
      "79980  0.81438      0      0           0  \n",
      "79981  0.53340      0      0           0  \n",
      "79982  0.96747      0      0           1  \n",
      "79983  1.00000      0      0           0  \n",
      "79984  0.75322      0      0           0  \n",
      "79985  0.85771      0      0           0  \n",
      "79986  0.86356      0      0           0  \n",
      "79987  0.41685      0      0           0  \n",
      "79988  0.53750      0      0           0  \n",
      "79989  0.00000      0      0           0  \n",
      "79990  0.84170      0      0           0  \n",
      "79991  0.44615      0      0           0  \n",
      "79992  0.73640      0      0           1  \n",
      "79993  0.97374      0      0           1  \n",
      "79994  0.79867      0      0           0  \n",
      "79995  0.43829      0      0           0  \n",
      "79996  0.57931      0      0           0  \n",
      "79997  0.42069      0      0           1  \n",
      "79998  0.53251      0      0           0  \n",
      "79999  0.68482      0      0           1  \n",
      "\n",
      "[80000 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "#replace all missing values with mean of existing values in training set\n",
    "#same can be done for testing set but data leakage can occur\n",
    "\n",
    "df_training = df_training.replace('missing', np.nan)\n",
    "df_training = df_training.replace('na', np.nan)\n",
    "df_training = df_training.replace('N/A', np.nan)\n",
    "\n",
    "df_training = df_training.fillna(0)\n",
    "\n",
    "print(\"data processed\")\n",
    "\n",
    "#df_training = (df_training - df_training.mean()) / (df_training.max() - df_training.min())\n",
    "\n",
    "#print(\"data normalised\")\n",
    "print(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of people: 80000\n",
      "Number of features: 46\n"
     ]
    }
   ],
   "source": [
    "#general informations\n",
    "\n",
    "n_people = df_training.shape[0]\n",
    "n_features = df_training.shape[1]-1\n",
    "print(\"Total number of people: {}\".format(n_people))\n",
    "print(\"Number of features: {}\".format(n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6', 'mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13', 'mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20', 'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27', 'mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34', 'mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41', 'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46']\n",
      "Target column: default_ind\n",
      "\n",
      "Feature values:-\n",
      "  mvar1   mvar2  mvar3  mvar4  mvar5 mvar6  mvar7 mvar8  mvar9 mvar10  ...    \\\n",
      "0  1696  1.6541  0.000    0.0    0.0     0   6015   322  40369  18414  ...     \n",
      "1  1846  0.8095  0.000    0.0    0.0   102   7532  3171  18234  13664  ...     \n",
      "2  1745  0.4001  0.000    0.0    0.0     0   2536     0      0   2536  ...     \n",
      "3  1739  0.2193  0.000    0.0    0.0  1982  26440  4955  20316  37013  ...     \n",
      "4  1787  0.0118  0.225    0.0    0.0  5451   5494  5494   7987   4696  ...     \n",
      "\n",
      "  mvar37 mvar38 mvar39   mvar40  mvar41   mvar42 mvar43   mvar44 mvar45 mvar46  \n",
      "0     10      4      1    73.78  82.547  0.08696     10  0.63899      0      0  \n",
      "1      0      2      0   99.129       0        0     13  0.63836      0      0  \n",
      "2      0      1      0        0   29.29        0      1  1.00000      0      0  \n",
      "3      3      2      0   96.272       0  0.15385      3  0.53241      0      0  \n",
      "4      3      2      0  115.019       0        0      1  0.92665      0      0  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mvar1       int64\n",
      "mvar2     float64\n",
      "mvar3     float64\n",
      "mvar4     float64\n",
      "mvar5     float64\n",
      "mvar6       int64\n",
      "mvar7       int64\n",
      "mvar8       int64\n",
      "mvar9       int64\n",
      "mvar10      int64\n",
      "mvar11      int64\n",
      "mvar12      int64\n",
      "mvar13      int64\n",
      "mvar14      int64\n",
      "mvar15      int64\n",
      "mvar16      int64\n",
      "mvar17      int64\n",
      "mvar18      int64\n",
      "mvar19      int64\n",
      "mvar20      int64\n",
      "mvar21    float64\n",
      "mvar22    float64\n",
      "mvar23    float64\n",
      "mvar24    float64\n",
      "mvar25      int64\n",
      "mvar26      int64\n",
      "mvar27      int64\n",
      "mvar28      int64\n",
      "mvar29      int64\n",
      "mvar30      int64\n",
      "mvar31      int64\n",
      "mvar32      int64\n",
      "mvar33    float64\n",
      "mvar34      int64\n",
      "mvar35      int64\n",
      "mvar36      int64\n",
      "mvar37      int64\n",
      "mvar38      int64\n",
      "mvar39      int64\n",
      "mvar40    float64\n",
      "mvar41    float64\n",
      "mvar42    float64\n",
      "mvar43      int64\n",
      "mvar44    float64\n",
      "mvar45      int64\n",
      "mvar46      int64\n",
      "dtype: object\n",
      "data normalised\n",
      "          mvar1     mvar2     mvar3     mvar4     mvar5     mvar6     mvar7  \\\n",
      "0      0.015102  0.014009 -0.018294 -0.004111 -0.004934 -0.008974 -0.001794   \n",
      "1      0.092025 -0.004916 -0.018294 -0.004111 -0.004934 -0.008227 -0.001520   \n",
      "2      0.040230 -0.014089 -0.018294 -0.004111 -0.004934 -0.008974 -0.002421   \n",
      "3      0.037153 -0.018140 -0.018294 -0.004111 -0.004934  0.005536  0.001887   \n",
      "4      0.061769 -0.022789 -0.017510 -0.004111 -0.004934  0.030932 -0.001888   \n",
      "5     -0.044898 -0.023054 -0.006086 -0.004111 -0.004934 -0.008974 -0.002878   \n",
      "6      0.077666 -0.014089 -0.018294 -0.004111 -0.004934 -0.008974 -0.002682   \n",
      "7     -0.854642 -0.023054 -0.018294 -0.004111 -0.004934 -0.008974 -0.002878   \n",
      "8      0.086897 -0.020011 -0.018294 -0.004111 -0.004934 -0.006434  0.004143   \n",
      "9      0.088435 -0.018615 -0.018294 -0.004111 -0.004934 -0.003169 -0.001773   \n",
      "10     0.121256 -0.023054 -0.018294 -0.004111 -0.004934 -0.008813  0.000639   \n",
      "11     0.007410  0.012546  0.001524  0.026980 -0.004934 -0.008293 -0.002237   \n",
      "12     0.089461 -0.010981 -0.018294 -0.004111 -0.004934  0.001912  0.000627   \n",
      "13     0.065358  0.021051 -0.018294 -0.004111 -0.004934 -0.002708 -0.001326   \n",
      "14     0.001256  0.063341  0.046075  0.050747  0.000363 -0.008974 -0.002852   \n",
      "15    -0.042847  0.044802  0.038615 -0.004111 -0.004934 -0.008974 -0.002878   \n",
      "16     0.119205 -0.020880 -0.018294 -0.004111 -0.004934 -0.001543 -0.000467   \n",
      "17     0.012025  0.173556  0.018089 -0.004111 -0.004934 -0.008081 -0.002765   \n",
      "18    -0.034642 -0.003782 -0.012978  0.009185 -0.004934 -0.008974 -0.002719   \n",
      "19     0.098692 -0.022706 -0.017011 -0.004111 -0.004934 -0.005870  0.012711   \n",
      "20     0.109461  0.035429 -0.018294 -0.004111 -0.004934 -0.008974  0.000591   \n",
      "21     0.095102  0.058060 -0.018294 -0.004111 -0.004934 -0.003125  0.000098   \n",
      "22    -0.005924  0.044766  0.033118 -0.004111 -0.004934 -0.008623 -0.002869   \n",
      "23     0.100743 -0.022155 -0.018294 -0.004111 -0.004934  0.001912 -0.001985   \n",
      "24     0.044333 -0.018010 -0.018294 -0.004111 -0.004934 -0.000746 -0.002675   \n",
      "25     0.116640 -0.022341 -0.018294 -0.004111 -0.004934  0.025105 -0.001538   \n",
      "26     0.069974 -0.014107 -0.018294 -0.004111 -0.004934 -0.005343 -0.000985   \n",
      "27     0.136128 -0.015713 -0.018294 -0.004111 -0.004934  0.042140  0.001009   \n",
      "28     0.024333  0.093245 -0.015610 -0.004111 -0.004934 -0.008974 -0.002537   \n",
      "29    -0.854642  0.022783 -0.002558 -0.004111 -0.004934 -0.008974 -0.002878   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "79970  0.085871  0.004202 -0.018294 -0.004111 -0.004934  0.007220 -0.001933   \n",
      "79971  0.032025  0.007634 -0.018294 -0.004111 -0.004934 -0.006368 -0.002118   \n",
      "79972  0.044333  0.107538 -0.018294 -0.004111 -0.004934 -0.007656 -0.002154   \n",
      "79973  0.041769  0.020941 -0.016056 -0.004111 -0.004934 -0.006844 -0.002253   \n",
      "79974  0.053563  0.016355 -0.018294 -0.004111 -0.004934 -0.008491 -0.000602   \n",
      "79975  0.050999 -0.001678 -0.018294 -0.004111 -0.004934 -0.000628 -0.001038   \n",
      "79976  0.079717 -0.023054 -0.011835  0.012044 -0.004934  0.009921 -0.000470   \n",
      "79977  0.080743 -0.010430 -0.018294 -0.004111 -0.004934  0.020046 -0.002034   \n",
      "79978  0.126897 -0.021835 -0.018294 -0.004111 -0.004934  0.016422  0.000803   \n",
      "79979 -0.034129  0.036657  0.045438  0.005288  0.006888 -0.008974 -0.001579   \n",
      "79980  0.079717 -0.009944 -0.018294 -0.004111 -0.004934 -0.008974  0.001390   \n",
      "79981  0.013563 -0.018958  0.043642  0.013535 -0.004934 -0.008140  0.002080   \n",
      "79982  0.035102  0.047977 -0.017388 -0.004111 -0.004934 -0.008118 -0.002717   \n",
      "79983  0.002281 -0.019962 -0.018294 -0.004111 -0.004934 -0.008974 -0.002789   \n",
      "79984  0.132025 -0.020318 -0.018294 -0.004111 -0.004934  0.009167  0.045177   \n",
      "79985  0.001769 -0.022276  0.015994 -0.004111 -0.004934 -0.007422 -0.001463   \n",
      "79986  0.120230 -0.019061 -0.018294 -0.004111 -0.004934  0.027301  0.001042   \n",
      "79987  0.086384 -0.011017 -0.018294 -0.004111 -0.004934 -0.008974  0.002478   \n",
      "79988  0.133563 -0.022135 -0.018294 -0.004111 -0.004934  0.152086  0.002536   \n",
      "79989 -0.854642 -0.019802 -0.018294 -0.004111 -0.004934 -0.008974 -0.002878   \n",
      "79990  0.140743 -0.012222 -0.018294 -0.004111 -0.004934  0.070428  0.006638   \n",
      "79991  0.119205 -0.023054 -0.018294 -0.004111 -0.004934  0.044542 -0.000261   \n",
      "79992  0.048435  0.023998 -0.018294 -0.004111 -0.004934 -0.008630 -0.000449   \n",
      "79993  0.017153 -0.014470 -0.018294 -0.004111 -0.004934 -0.006456 -0.002816   \n",
      "79994  0.124846 -0.023054 -0.018294 -0.004111 -0.004934  0.027301  0.005647   \n",
      "79995  0.035615  0.025658 -0.018294 -0.004111 -0.004934 -0.008894 -0.002112   \n",
      "79996  0.029461 -0.023054 -0.014432  0.002585 -0.004934 -0.008974  0.008662   \n",
      "79997 -0.031565 -0.016554  0.022008  0.004058  0.008204 -0.008974 -0.002467   \n",
      "79998  0.058179  0.003552 -0.018294 -0.004111 -0.004934 -0.008974 -0.001732   \n",
      "79999  0.030999  0.020164 -0.013271 -0.004111 -0.004934 -0.008974  0.001767   \n",
      "\n",
      "          mvar8     mvar9    mvar10    ...       mvar37    mvar38    mvar39  \\\n",
      "0     -0.016607  0.003000 -0.002110    ...     0.102636 -0.002732  0.043621   \n",
      "1     -0.006843 -0.002964 -0.002953    ...    -0.064031 -0.023784 -0.003998   \n",
      "2     -0.017710 -0.007876 -0.004929    ...    -0.064031 -0.034311 -0.003998   \n",
      "3     -0.000730 -0.002403  0.001191    ...    -0.014031 -0.023784 -0.003998   \n",
      "4      0.001117 -0.005724 -0.004545    ...    -0.014031 -0.023784 -0.003998   \n",
      "5     -0.017710 -0.007876 -0.005379    ...    -0.064031 -0.023784 -0.003998   \n",
      "6     -0.017710 -0.007462 -0.005113    ...    -0.064031 -0.044837 -0.003998   \n",
      "7     -0.017710 -0.007876 -0.005379    ...    -0.064031 -0.044837 -0.003998   \n",
      "8      0.043384  0.011179  0.006309    ...     0.135969 -0.023784 -0.003998   \n",
      "9      0.003005  0.005314  0.000238    ...     0.052636 -0.044837 -0.003998   \n",
      "10     0.016250  0.016537  0.014196    ...     0.119303 -0.023784 -0.003998   \n",
      "11    -0.016206 -0.007195 -0.004121    ...    -0.064031  0.007794  0.043621   \n",
      "12     0.048939 -0.006061 -0.004205    ...    -0.064031  0.018321 -0.003998   \n",
      "13     0.011792  0.008435 -0.000960    ...     0.035969 -0.034311 -0.003998   \n",
      "14    -0.017710 -0.007866 -0.005354    ...    -0.064031  0.113058 -0.003998   \n",
      "15    -0.017710  0.005985 -0.005379    ...     0.219302 -0.002732 -0.003998   \n",
      "16     0.028138  0.058642  0.004657    ...     0.369302 -0.023784 -0.003998   \n",
      "17    -0.016685 -0.007569 -0.005016    ...    -0.047364  0.081479 -0.003998   \n",
      "18    -0.017710 -0.007876 -0.005223    ...    -0.064031 -0.034311  0.043621   \n",
      "19     0.024958  0.015464  0.019843    ...     0.102636 -0.023784 -0.003998   \n",
      "20     0.004023  0.008181  0.010048    ...    -0.047364  0.102531 -0.003998   \n",
      "21     0.038885  0.021934  0.001954    ...     0.019303 -0.023784 -0.003998   \n",
      "22    -0.017546 -0.007823 -0.005370    ...    -0.064031 -0.002732 -0.003998   \n",
      "23    -0.000730 -0.001706 -0.001514    ...     0.035969 -0.002732 -0.003998   \n",
      "24    -0.013858 -0.007876 -0.005179    ...    -0.064031 -0.034311 -0.003998   \n",
      "25     0.007762 -0.000961 -0.003187    ...    -0.014031 -0.034311 -0.003998   \n",
      "26    -0.016010 -0.007609 -0.002155    ...    -0.047364 -0.034311 -0.003998   \n",
      "27     0.046815  0.042559  0.014825    ...     0.119303 -0.023784 -0.003998   \n",
      "28    -0.015671 -0.004489 -0.004522    ...     0.035969  0.039373 -0.003998   \n",
      "29    -0.017710 -0.007876 -0.005379    ...    -0.064031 -0.002732 -0.003998   \n",
      "...         ...       ...       ...    ...          ...       ...       ...   \n",
      "79970 -0.003307 -0.003365 -0.002924    ...    -0.064031  0.039373 -0.003998   \n",
      "79971 -0.010918 -0.002850 -0.003795    ...    -0.030697  0.028847 -0.003998   \n",
      "79972 -0.015596 -0.006888 -0.003642    ...    -0.064031  0.060426 -0.003998   \n",
      "79973 -0.005822 -0.004339 -0.002824    ...    -0.030697  0.081479 -0.003998   \n",
      "79974  0.004335 -0.000347 -0.001047    ...    -0.064031 -0.002732 -0.003998   \n",
      "79975  0.017268  0.006941 -0.001716    ...    -0.030697  0.092005 -0.003998   \n",
      "79976  0.002666  0.000480  0.008250    ...     0.002636 -0.002732 -0.003998   \n",
      "79977 -0.001662 -0.007876 -0.003844    ...    -0.047364 -0.023784 -0.003998   \n",
      "79978  0.052281  0.013346 -0.000434    ...     0.069303 -0.034311 -0.003998   \n",
      "79979 -0.017710 -0.007619 -0.004072    ...    -0.030697  0.049900  0.043621   \n",
      "79980  0.063456  0.048207  0.004512    ...     0.202636 -0.013258 -0.003998   \n",
      "79981 -0.016394 -0.007249  0.003156    ...     0.085969 -0.002732 -0.003998   \n",
      "79982 -0.017309 -0.006862 -0.005154    ...    -0.064031 -0.013258 -0.003998   \n",
      "79983 -0.017710 -0.007876 -0.005292    ...    -0.064031 -0.034311 -0.003998   \n",
      "79984  0.896183  0.101314  0.063951    ...     0.152636 -0.002732 -0.003998   \n",
      "79985 -0.016027  0.000148 -0.002499    ...     0.069303  0.007794 -0.003998   \n",
      "79986  0.025719  0.011061  0.005801    ...     0.019303  0.007794 -0.003998   \n",
      "79987 -0.004126  0.010112  0.009870    ...     0.019303 -0.013258 -0.003998   \n",
      "79988  0.065101  0.012334  0.010303    ...     0.002636 -0.013258 -0.003998   \n",
      "79989 -0.017710 -0.007876 -0.005379    ...    -0.064031 -0.034311 -0.003998   \n",
      "79990  0.103869  0.028887  0.025906    ...     0.085969 -0.034311 -0.003998   \n",
      "79991  0.032045 -0.006461 -0.004500    ...     0.085969 -0.034311 -0.003998   \n",
      "79992 -0.011494 -0.005365 -0.004935    ...    -0.014031  0.007794 -0.003998   \n",
      "79993 -0.016531 -0.007609 -0.005318    ...    -0.064031 -0.013258 -0.003998   \n",
      "79994  0.144402  0.015865  0.013642    ...    -0.030697 -0.023784 -0.003998   \n",
      "79995 -0.012306 -0.004272 -0.004195    ...    -0.064031 -0.002732 -0.003998   \n",
      "79996 -0.017710 -0.004933  0.009681    ...    -0.030697  0.007794 -0.003998   \n",
      "79997 -0.017710 -0.006808 -0.004365    ...    -0.064031  0.007794  0.091240   \n",
      "79998 -0.001254 -0.007012 -0.002152    ...    -0.064031  0.039373 -0.003998   \n",
      "79999 -0.007878  0.027951  0.007365    ...     0.185969 -0.002732 -0.003998   \n",
      "\n",
      "         mvar40    mvar41    mvar42    mvar43    mvar44    mvar45    mvar46  \n",
      "0      0.121552  0.350803 -0.120109  0.038618  0.001985 -0.006215 -0.003618  \n",
      "1      0.180462 -0.134768 -0.163589  0.071585  0.001355 -0.006215 -0.003618  \n",
      "2     -0.049910  0.037526 -0.163589 -0.060283  0.362995 -0.006215 -0.003618  \n",
      "3      0.173823 -0.134768 -0.086664 -0.038305 -0.104595 -0.006215 -0.003618  \n",
      "4      0.217390 -0.134768 -0.163589 -0.060283  0.289645 -0.006215 -0.003618  \n",
      "5     -0.049910 -0.134768  0.586411 -0.071272 -0.637005 -0.006215 -0.003618  \n",
      "6      0.154996 -0.134768 -0.163589 -0.049294  0.235235 -0.006215 -0.003618  \n",
      "7     -0.049910 -0.134768 -0.163589 -0.071272 -0.637005 -0.006215 -0.003618  \n",
      "8     -0.049910 -0.134768 -0.163589  0.038618  0.261675 -0.006215 -0.003618  \n",
      "9     -0.049910  0.133409 -0.119469  0.082574 -0.298665 -0.006215 -0.003618  \n",
      "10    -0.049910 -0.134768 -0.163589  0.115541  0.169195 -0.006215 -0.003618  \n",
      "11    -0.049910 -0.134768  0.141966  0.005651  0.174715 -0.006215 -0.003618  \n",
      "12    -0.049910 -0.134768 -0.163589 -0.016327  0.343305 -0.006215 -0.003618  \n",
      "13    -0.049910 -0.134768 -0.163589  0.071585 -0.279765 -0.006215 -0.003618  \n",
      "14     0.200671  0.445609  0.116411  0.049607 -0.345315 -0.006215  0.049013  \n",
      "15    -0.049910 -0.134768 -0.001824 -0.049294 -0.637005 -0.006215 -0.003618  \n",
      "16    -0.025778 -0.134768 -0.163589  0.137519 -0.162205 -0.006215 -0.003618  \n",
      "17    -0.049910 -0.134768  0.154591 -0.016327  0.312505 -0.006215 -0.003618  \n",
      "18    -0.049910  0.401556  0.236411 -0.071272  0.362995 -0.006215 -0.003618  \n",
      "19    -0.049910 -0.134768 -0.163589  0.060596 -0.289705 -0.006215 -0.003618  \n",
      "20    -0.049910  0.291685 -0.163589  0.082574 -0.349925 -0.006215 -0.003618  \n",
      "21    -0.049910 -0.134768 -0.163589  0.071585  0.116485 -0.006215 -0.003618  \n",
      "22    -0.049910 -0.134768  0.336411 -0.060283  0.362995 -0.006215 -0.003618  \n",
      "23     0.175270 -0.134768 -0.163589  0.060596 -0.228475 -0.006215 -0.003618  \n",
      "24    -0.049910 -0.134768 -0.163589 -0.071272  0.362995 -0.006215 -0.003618  \n",
      "25    -0.049910 -0.134768 -0.163589 -0.038305  0.357185 -0.006215 -0.003618  \n",
      "26     0.163318  0.378797 -0.063589 -0.016327  0.115885 -0.006215 -0.003618  \n",
      "27    -0.049910 -0.134768 -0.163589  0.049607  0.360095 -0.006215 -0.003618  \n",
      "28    -0.049910 -0.134768  0.014981 -0.005338  0.213815 -0.006215 -0.003618  \n",
      "29    -0.049910 -0.134768  0.419746 -0.071272 -0.637005 -0.006215 -0.003618  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "79970 -0.049910  0.438161 -0.163589  0.016640 -0.056605 -0.006215 -0.003618  \n",
      "79971 -0.049910  0.357367 -0.118134 -0.005338 -0.010685 -0.006215 -0.003618  \n",
      "79972 -0.049910  0.377550 -0.108034  0.027629 -0.043365 -0.006215 -0.003618  \n",
      "79973 -0.049910  0.407844  0.056411  0.082574  0.201085 -0.006215 -0.003618  \n",
      "79974 -0.049910  0.103332 -0.163589  0.038618 -0.168375 -0.006215 -0.003618  \n",
      "79975 -0.049910 -0.134768 -0.138589  0.082574 -0.089465 -0.006215 -0.003618  \n",
      "79976  0.121159  0.127914 -0.146349  0.148508 -0.405235 -0.006215 -0.003618  \n",
      "79977 -0.049910 -0.134768 -0.163589 -0.071272  0.035215 -0.006215 -0.003618  \n",
      "79978 -0.049910 -0.134768 -0.163589  0.016640  0.362995 -0.006215 -0.003618  \n",
      "79979  0.257632  0.199744  0.112726  0.082574 -0.519375 -0.006215 -0.003618  \n",
      "79980 -0.049910 -0.134768 -0.163589  0.115541  0.177375 -0.006215 -0.003618  \n",
      "79981  0.123955 -0.134768 -0.074304 -0.016327 -0.103605 -0.006215 -0.003618  \n",
      "79982 -0.049910  0.447738 -0.163589 -0.049294  0.330465 -0.006215 -0.003618  \n",
      "79983 -0.049910 -0.134768  0.236411 -0.049294  0.362995 -0.006215 -0.003618  \n",
      "79984 -0.049910  0.281697 -0.163589  0.071585  0.116215 -0.006215 -0.003618  \n",
      "79985 -0.049910  0.328173 -0.095409  0.038618  0.220705 -0.006215 -0.003618  \n",
      "79986 -0.049910  0.225826 -0.163589  0.038618  0.226555 -0.006215 -0.003618  \n",
      "79987 -0.049910 -0.134768 -0.163589  0.027629 -0.220155 -0.006215 -0.003618  \n",
      "79988 -0.049910 -0.020391 -0.163589  0.005651 -0.099505 -0.006215 -0.003618  \n",
      "79989 -0.049910 -0.134768  0.586411 -0.071272 -0.637005 -0.006215 -0.003618  \n",
      "79990 -0.049910 -0.134768 -0.134179  0.016640  0.204695 -0.006215 -0.003618  \n",
      "79991 -0.049910 -0.134768 -0.163589 -0.038305 -0.190855 -0.006215 -0.003618  \n",
      "79992 -0.049910 -0.134768 -0.101089 -0.016327  0.099395 -0.006215 -0.003618  \n",
      "79993 -0.049910 -0.134768  0.169746 -0.049294  0.336735 -0.006215 -0.003618  \n",
      "79994 -0.049910 -0.134768 -0.163589 -0.027316  0.161665 -0.006215 -0.003618  \n",
      "79995 -0.049910  0.326279 -0.163589 -0.027316 -0.198715 -0.006215 -0.003618  \n",
      "79996 -0.049910  0.090673 -0.080254  0.082574 -0.057695 -0.006215 -0.003618  \n",
      "79997  0.186786  0.413126  0.086411 -0.027316 -0.216315 -0.006215 -0.003618  \n",
      "79998 -0.049910  0.318303 -0.133284  0.027629 -0.104495 -0.006215 -0.003618  \n",
      "79999  0.025387 -0.134768 -0.127874  0.060596  0.047815 -0.006215 -0.003618  \n",
      "\n",
      "[80000 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "#preparing data\n",
    "\n",
    "# Extract feature (X) and target (y) columns\n",
    "var47 = df_training.columns\n",
    "feature_cols = list(df_training.columns[:-1])  # all columns but last are features\n",
    "target_col = df_training.columns[-1] # last column is the target/label\n",
    "print(\"Feature column(s):-\\n{}\".format(feature_cols))\n",
    "print( \"Target column: {}\".format(target_col))\n",
    "\n",
    "X_all = df_training[feature_cols]  # feature values for all people\n",
    "y_all = df_training[target_col]  # corresponding targets/labels\n",
    "\n",
    "print (\"\\nFeature values:-\")\n",
    "print(X_all.head())  # print the first 5 rows\n",
    "\n",
    "X_all = X_all.convert_objects(convert_numeric=True)\n",
    "\n",
    "\n",
    "for head in X_all:\n",
    "    series = X_all[head]\n",
    "    mean = series.mean()\n",
    "    maximum = series.max()\n",
    "    minimum = series.min()\n",
    "    series = (series - mean)/(maximum - minimum)\n",
    "    X_all[head] = series\n",
    "    \n",
    "\n",
    "print(\"data normalised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        C\n",
      "1        L\n",
      "2        C\n",
      "3        L\n",
      "4        L\n",
      "5        C\n",
      "6        C\n",
      "7        C\n",
      "8        L\n",
      "9        L\n",
      "10       L\n",
      "11       C\n",
      "12       C\n",
      "13       C\n",
      "14       C\n",
      "15       C\n",
      "16       L\n",
      "17       L\n",
      "18       C\n",
      "19       L\n",
      "20       L\n",
      "21       L\n",
      "22       C\n",
      "23       L\n",
      "24       C\n",
      "25       L\n",
      "26       L\n",
      "27       C\n",
      "28       L\n",
      "29       L\n",
      "        ..\n",
      "79970    C\n",
      "79971    C\n",
      "79972    L\n",
      "79973    L\n",
      "79974    C\n",
      "79975    C\n",
      "79976    C\n",
      "79977    C\n",
      "79978    L\n",
      "79979    L\n",
      "79980    L\n",
      "79981    C\n",
      "79982    C\n",
      "79983    C\n",
      "79984    L\n",
      "79985    L\n",
      "79986    L\n",
      "79987    L\n",
      "79988    L\n",
      "79989    L\n",
      "79990    L\n",
      "79991    L\n",
      "79992    C\n",
      "79993    C\n",
      "79994    L\n",
      "79995    C\n",
      "79996    C\n",
      "79997    C\n",
      "79998    L\n",
      "79999    C\n",
      "Name: mvar47, Length: 80000, dtype: object\n",
      "0        1\n",
      "1        0\n",
      "2        1\n",
      "3        0\n",
      "4        0\n",
      "5        1\n",
      "6        1\n",
      "7        1\n",
      "8        0\n",
      "9        0\n",
      "10       0\n",
      "11       1\n",
      "12       1\n",
      "13       1\n",
      "14       1\n",
      "15       1\n",
      "16       0\n",
      "17       0\n",
      "18       1\n",
      "19       0\n",
      "20       0\n",
      "21       0\n",
      "22       1\n",
      "23       0\n",
      "24       1\n",
      "25       0\n",
      "26       0\n",
      "27       1\n",
      "28       0\n",
      "29       0\n",
      "        ..\n",
      "79970    1\n",
      "79971    1\n",
      "79972    0\n",
      "79973    0\n",
      "79974    1\n",
      "79975    1\n",
      "79976    1\n",
      "79977    1\n",
      "79978    0\n",
      "79979    0\n",
      "79980    0\n",
      "79981    1\n",
      "79982    1\n",
      "79983    1\n",
      "79984    0\n",
      "79985    0\n",
      "79986    0\n",
      "79987    0\n",
      "79988    0\n",
      "79989    0\n",
      "79990    0\n",
      "79991    0\n",
      "79992    1\n",
      "79993    1\n",
      "79994    0\n",
      "79995    1\n",
      "79996    1\n",
      "79997    1\n",
      "79998    0\n",
      "79999    1\n",
      "Name: mvar47, Length: 80000, dtype: int64\n",
      "Processed feature columns (47):-\n",
      "['mvar1', 'mvar2', 'mvar3', 'mvar4', 'mvar5', 'mvar6', 'mvar7', 'mvar8', 'mvar9', 'mvar10', 'mvar11', 'mvar12', 'mvar13', 'mvar14', 'mvar15', 'mvar16', 'mvar17', 'mvar18', 'mvar19', 'mvar20', 'mvar21', 'mvar22', 'mvar23', 'mvar24', 'mvar25', 'mvar26', 'mvar27', 'mvar28', 'mvar29', 'mvar30', 'mvar31', 'mvar32', 'mvar33', 'mvar34', 'mvar35', 'mvar36', 'mvar37', 'mvar38', 'mvar39', 'mvar40', 'mvar41', 'mvar42', 'mvar43', 'mvar44', 'mvar45', 'mvar46', 'mvar47']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['C', 'L'], [1, 0])\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        #if col_data.dtype == object:\n",
    "            #col_data = pd.get_dummies(col_data, prefix=col)\n",
    "\n",
    "        outX = outX.join(col_data)  # collect columns in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "mvar47 = mvar47.replace('C',1)\n",
    "mvar47 = mvar47.replace('L',0)\n",
    "\n",
    "X_all = pd.concat((X_all,mvar47),axis =1)\n",
    "\n",
    "print(\"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 64000 samples\n",
      "Test set: 16000 samples\n"
     ]
    }
   ],
   "source": [
    "# preparing number of training and test samples\n",
    "num_all = df_training.shape[0]  # same as len(df_training)\n",
    "num_train = 64000 # about 90% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "from sklearn import cross_validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, test_size=num_test)\n",
    "print(\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Test set: {} samples\".format(X_test.shape[0]))\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoostingClassifier...\n",
      "Done!\n",
      "Training time (secs): 28.395\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print(\"Training {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nTraining time (secs): {:.3f}\".format(end - start))\n",
    "\n",
    "from sklearn import ensemble\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.184\n",
      "F1 score for training set: 0.5485775221845602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print(\"Predicting labels using {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print( \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label=1)\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.071\n",
      "F1 score for test set: 0.5239548367409216\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print(\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "\n",
    "#Now just by changing the classifier we can compare the f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training GradientBoostingClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.134\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.049\n",
      "F1 score for test set: 0.4463072698366816\n",
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training GradientBoostingClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.098\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using GradientBoostingClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.065\n",
      "F1 score for test set: 0.300443837487197\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"Training set size: {}\".format(len(X_train)))\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    print( \"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test)))\n",
    "\n",
    "train_predict(clf, X_train.sample(n=200, random_state=200), y_train.sample(n=200, random_state=200), X_test, y_test)\n",
    "train_predict(clf, X_train.sample(n=100, random_state=100), y_train.sample(n=100, random_state=100), X_test, y_test)\n",
    "# Note: Keep the test set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 64000\n",
      "Training MultinomialNB...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-a626e14ab25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m201\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m201\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m101\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-a55ebac4b583>\u001b[0m in \u001b[0;36mtrain_predict\u001b[1;34m(clf, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training set size: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"F1 score for training set: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"F1 score for test set: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-c8a69201361d>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[1;34m(clf, X_train, y_train)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training {}...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\\nTraining time (secs): {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[0;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "mnb = naive_bayes.MultinomialNB()\n",
    "train_predict(mnb, X_train, y_train, X_test, y_test)\n",
    "train_predict(mnb, X_train.sample(n=200, random_state=201), y_train.sample(n=200, random_state=201), X_test, y_test)\n",
    "train_predict(mnb, X_train.sample(n=100, random_state=101), y_train.sample(n=100, random_state=101), X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 64000\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 4.966\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.107\n",
      "F1 score for training set: 0.9999365119674941\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.028\n",
      "F1 score for test set: 0.46387643248629795\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.010\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.023\n",
      "F1 score for test set: 0.3890084842345194\n",
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.018\n",
      "F1 score for test set: 0.441117529175999\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "train_predict(dtc, X_train, y_train, X_test, y_test)\n",
    "train_predict(dtc, X_train.sample(n=200, random_state=202), y_train.sample(n=200, random_state=202), X_test, y_test)\n",
    "train_predict(dtc, X_train.sample(n=100, random_state=102), y_train.sample(n=100, random_state=102), X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing the best model\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 'n_estimators': (100, 125, 150, 500)}\n",
    "sss = cross_validation.StratifiedShuffleSplit(y_train, test_size=num_test)\n",
    "gs = GridSearchCV(estimator=clf, n_jobs=-1, scoring=make_scorer(f1_score, pos_label=1), param_grid=parameters,\n",
    "                  cv=sss)\n",
    "gs.fit(X_train, y_train)\n",
    "best_estimator = gs.best_estimator_\n",
    "print(\"best estimator:\\n{}\".format(best_estimator))\n",
    "print ('')\n",
    "print(\"best parameter:\\n{}\".format(gs.best_params_))\n",
    "print('')\n",
    "print(\"F1 score:\\n{}\".format(f1_score(y_test, best_estimator.predict(X_test), pos_label=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
